# nue

![workflow](https://github.com/ishikawa/nue/actions/workflows/nue.yml/badge.svg)

> The homebrew-scale LLM ğŸµğŸ¦ğŸ¯ğŸ

> [!WARNING]
> Work in progress â€” nothing useful here yet.

I'd like to gain practical experience with transformers, particularly by understanding their architecture and real-world applications, with a focus on small-scale LLMs. To achieve this, I decided to create _a tiny LLM_. My goal is to integrate it into web applications, games, and iOS apps that interest me.

## Goal

Build a small language model that generates grammatically correct sentences.

## Philosophy

Keep it simple, but not simplistic.

## Dataset

| åå‰                                                                              | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹                                                            | å‚™è€ƒ                                                                                                          |
| --------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| [Wikimedia Wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia)        | [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)       |                                                                                                               |
| [livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹](https://www.rondhuit.com/download.html#news%20corpus) | [CC BY-ND 2.1 JP](https://creativecommons.org/licenses/by-nd/2.1/jp/) | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) |

## Tokenizer

æ—¥è‹±ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¦ SentencePiece + Unigram ã§å­¦ç¿’ã—ã¾ã™ã€‚

- `byte_fallback=True` ã§ OOV (èªå½™å¤–) å›é¿
- `vocab_size` ã¯ 32,000

(1) ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”Ÿæˆ

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ `build/corpus.txt` ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

```
$ poetry run nue build-corpus
```

(2) Tokenizer ã‚’å­¦ç¿’

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ `build/tokenizer.model` ã¨ `build/tokenizer.vocab` ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

```
$ poetry run nue train-tokenizer
```

## Training

```
poetry run nue train
```

## License

Apache License 2.0

## Name origin ğŸµğŸ¦ğŸ¯ğŸ

The name Nue (éµº, pronounced "noo-eh") originates from Japanese folklore, referring to a mythical creature with the head of a monkey, body of a tanuki (raccoon dog), legs of a tiger, and tail of a snake.

Inspired by this creature, this project serves as a personal exploration into building language modelsâ€”combining newly-learned, diverse techniques into one cohesive yet eclectic model. Additionally, "Nue" can be read similarly to "new," symbolizing this as a new, homebrew-scale LLM crafted from scratch.

## References

Excellent articles and papers that I've read:

- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT in 60 Lines of NumPy | Jay Mody](https://jaykmody.com/blog/gpt-from-scratch/)
- [Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 Â· karpathy/llm.c Â· Discussion #481](https://github.com/karpathy/llm.c/discussions/481)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) a.k.a. _"Chinchilla Scaling Law"_
- [Physics of Language Models](https://physics.allen-zhu.com/home)
